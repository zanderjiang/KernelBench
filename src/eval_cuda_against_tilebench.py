"""
Evaluate CUDA/Triton kernels generated by KernelBench against TileBench references

Flow:
1. Load TileBench reference.py (get inputs and ground truth)
2. Load generated kernel and extract 'run' function
3. Compare outputs and measure performance

Expected generated kernel format:
    def run(**inputs):
        # CUDA/Triton implementation
        return output
"""

import importlib.util
import os
import sys
import tempfile
import time
import traceback
from typing import Any, Dict, Optional, Callable

import torch
from pydantic import BaseModel

from src.tilebench_reference import load_tilebench_reference


class KernelEvalResult(BaseModel):
    """Evaluation result for a generated kernel against TileBench reference"""
    
    compiled: bool = False
    correctness: bool = False
    runtime_ms: float = -1.0
    reference_runtime_ms: float = -1.0
    speedup: float = 0.0
    error_type: Optional[str] = None
    error_message: Optional[str] = None
    metadata: Dict[str, Any] = {}


def load_cuda_kernel_inline(
    kernel_source: str,
    kernel_name: str = "run",
) -> Callable:
    """
    Load CUDA kernel using torch.utils.cpp_extension.load_inline
    
    Expects kernel_source to contain:
    - CUDA C++ code with __global__ kernels
    - Python bindings via pybind11 exposing 'run' function
    
    Returns callable kernel function
    """
    from torch.utils.cpp_extension import load_inline
    
    # Extract CUDA source and binding code
    # Assume format with ///CUDA_SOURCE and ///BINDING sections or similar
    
    cuda_source = ""
    cpp_source = ""
    
    if "//CUDA_SOURCE" in kernel_source and "//BINDING" in kernel_source:
        parts = kernel_source.split("//BINDING")
        cuda_source = parts[0].replace("//CUDA_SOURCE", "").strip()
        cpp_source = parts[1].strip()
    else:
        # Try to parse as single file with both
        cuda_source = kernel_source
        cpp_source = ""
    
    # Load the kernel
    module = load_inline(
        name=f"custom_kernel_{kernel_name}",
        cpp_sources=[cpp_source] if cpp_source else [],
        cuda_sources=[cuda_source],
        functions=[kernel_name],
        with_cuda=True,
        build_directory=tempfile.mkdtemp(),
    )
    
    return getattr(module, kernel_name)


def load_triton_kernel(kernel_source: str, kernel_name: str = "run") -> Callable:
    """
    Load Triton kernel from source code
    
    Expects kernel_source to be valid Triton Python code with @triton.jit decorator
    Returns callable kernel function
    """
    # Create temporary module
    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
        f.write(kernel_source)
        temp_path = f.name
    
    try:
        spec = importlib.util.spec_from_file_location("triton_kernel", temp_path)
        module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(module)
        
        # Find the kernel function
        if hasattr(module, kernel_name):
            return getattr(module, kernel_name)
        
        # Try to find any callable that looks like a kernel
        for attr_name in dir(module):
            if not attr_name.startswith('_'):
                attr = getattr(module, attr_name)
                if callable(attr):
                    return attr
        
        raise AttributeError(f"No callable kernel found in Triton source")
    
    finally:
        try:
            os.unlink(temp_path)
        except Exception:
            pass


def load_generated_kernel(
    kernel_source: str,
    backend: str = "cuda",
    kernel_name: str = "run",
) -> Callable:
    """
    Load generated kernel based on backend
    
    Args:
        kernel_source: Source code string
        backend: "cuda", "triton", or "python"
        kernel_name: Name of the kernel function (default: "run")
    
    Returns:
        Callable kernel function that takes same inputs as TileBench reference
    """
    if backend == "triton":
        return load_triton_kernel(kernel_source, kernel_name)
    elif backend == "cuda":
        return load_cuda_kernel_inline(kernel_source, kernel_name)
    elif backend == "python":
        # Pure Python/PyTorch implementation
        namespace = {}
        exec(kernel_source, namespace)
        if kernel_name in namespace:
            return namespace[kernel_name]
        raise AttributeError(f"Function '{kernel_name}' not found in source")
    else:
        raise ValueError(f"Unknown backend: {backend}")


def eval_kernel_against_tilebench(
    reference_path: str,
    kernel_source: str,
    backend: str = "cuda",
    kernel_name: str = "run",
    config: Optional[Dict] = None,
    warmup: int = 3,
    repeat: int = 20,
    test_all_shapes: bool = False,
) -> KernelEvalResult:
    """
    Evaluate a generated kernel against TileBench reference
    
    Args:
        reference_path: Path to TileBench reference.py
        kernel_source: Generated kernel source code
        backend: "cuda", "triton", or "python"
        kernel_name: Name of kernel function to call (default: "run")
        config: Optional config override (if None and test_all_shapes=False, uses default)
        warmup: Warmup iterations
        repeat: Benchmark iterations
        test_all_shapes: If True and reference has get_shape_suites(), test on all configs
    
    Returns:
        KernelEvalResult with metrics (aggregated if test_all_shapes=True)
    """
    result = KernelEvalResult()
    
    try:
        # 1. Load reference
        ref_module = load_tilebench_reference(reference_path)
        
        # 2. Get config and inputs
        if config is None:
            config = ref_module.get_default_config()
        
        inputs = ref_module.make_inputs(config)
        
        # 3. Get reference output
        if isinstance(inputs, dict):
            ref_output = ref_module.reference(**inputs)
        else:
            ref_output = ref_module.reference(inputs)
        
        # 4. Load generated kernel
        try:
            kernel_fn = load_generated_kernel(kernel_source, backend, kernel_name)
            result.compiled = True
        except Exception as e:
            result.error_type = type(e).__name__
            result.error_message = f"Compilation failed: {str(e)}"
            result.metadata["traceback"] = traceback.format_exc()
            return result
        
        # 5. Run kernel and check correctness
        try:
            if isinstance(inputs, dict):
                kernel_output = kernel_fn(**inputs)
            else:
                kernel_output = kernel_fn(inputs)
            
            # Check correctness
            atol = config.get("atol", 1e-2)
            rtol = config.get("rtol", 1e-2)
            
            if hasattr(ref_module, "check"):
                try:
                    ref_module.check(ref_output, kernel_output, atol=atol, rtol=rtol)
                    result.correctness = True
                except AssertionError as e:
                    result.correctness = False
                    result.error_message = f"Check failed: {str(e)}"
            else:
                # Fallback to torch.allclose
                if isinstance(ref_output, torch.Tensor) and isinstance(kernel_output, torch.Tensor):
                    result.correctness = torch.allclose(
                        kernel_output, ref_output, atol=atol, rtol=rtol
                    )
                    if not result.correctness:
                        max_diff = (kernel_output - ref_output).abs().max().item()
                        result.error_message = f"Outputs differ: max diff = {max_diff}"
                else:
                    result.correctness = (kernel_output == ref_output)
        
        except Exception as e:
            result.correctness = False
            result.error_type = type(e).__name__
            result.error_message = f"Runtime error: {str(e)}"
            result.metadata["traceback"] = traceback.format_exc()
            return result
        
        # 6. Benchmark if correct
        if result.correctness:
            try:
                # Warmup
                for _ in range(warmup):
                    if isinstance(inputs, dict):
                        _ = kernel_fn(**inputs)
                    else:
                        _ = kernel_fn(inputs)
                    if torch.cuda.is_available():
                        torch.cuda.synchronize()
                
                # Benchmark kernel
                kernel_times = []
                for _ in range(repeat):
                    if torch.cuda.is_available():
                        start = torch.cuda.Event(enable_timing=True)
                        end = torch.cuda.Event(enable_timing=True)
                        start.record()
                        
                        if isinstance(inputs, dict):
                            _ = kernel_fn(**inputs)
                        else:
                            _ = kernel_fn(inputs)
                        
                        end.record()
                        torch.cuda.synchronize()
                        kernel_times.append(start.elapsed_time(end))
                    else:
                        start = time.perf_counter()
                        if isinstance(inputs, dict):
                            _ = kernel_fn(**inputs)
                        else:
                            _ = kernel_fn(inputs)
                        end = time.perf_counter()
                        kernel_times.append((end - start) * 1000)
                
                result.runtime_ms = sum(kernel_times) / len(kernel_times)
                
                # Benchmark reference
                ref_times = []
                for _ in range(repeat):
                    if torch.cuda.is_available():
                        start = torch.cuda.Event(enable_timing=True)
                        end = torch.cuda.Event(enable_timing=True)
                        start.record()
                        
                        if isinstance(inputs, dict):
                            _ = ref_module.reference(**inputs)
                        else:
                            _ = ref_module.reference(inputs)
                        
                        end.record()
                        torch.cuda.synchronize()
                        ref_times.append(start.elapsed_time(end))
                    else:
                        start = time.perf_counter()
                        if isinstance(inputs, dict):
                            _ = ref_module.reference(**inputs)
                        else:
                            _ = ref_module.reference(inputs)
                        end = time.perf_counter()
                        ref_times.append((end - start) * 1000)
                
                result.reference_runtime_ms = sum(ref_times) / len(ref_times)
                result.speedup = result.reference_runtime_ms / result.runtime_ms if result.runtime_ms > 0 else 0.0
                
            except Exception as e:
                result.error_message = f"Benchmarking error: {str(e)}"
                result.metadata["benchmark_traceback"] = traceback.format_exc()
    
    except Exception as e:
        result.error_type = type(e).__name__
        result.error_message = str(e)
        result.metadata["traceback"] = traceback.format_exc()
    
    return result


def eval_kernel_on_all_shapes(
    reference_path: str,
    kernel_source: str,
    backend: str = "cuda",
    kernel_name: str = "run",
    warmup: int = 3,
    repeat: int = 10,
) -> Dict[str, KernelEvalResult]:
    """
    Evaluate kernel on all shape configurations from get_shape_suites()
    
    Args:
        reference_path: Path to TileBench reference.py
        kernel_source: Generated kernel source code
        backend: "cuda", "triton", or "python"
        kernel_name: Name of kernel function (default: "run")
        warmup: Warmup iterations per config
        repeat: Benchmark repetitions per config
        
    Returns:
        Dict mapping config description to evaluation results
    """
    # Load reference to get shape suites
    ref_module = load_tilebench_reference(reference_path)
    
    results = {}
    
    # Check if get_shape_suites exists
    if not hasattr(ref_module, "get_shape_suites"):
        # Fall back to single default config
        result = eval_kernel_against_tilebench(
            reference_path, kernel_source, backend, kernel_name,
            config=None, warmup=warmup, repeat=repeat
        )
        results["default"] = result
        return results
    
    # Get all shape configurations
    shape_suites = ref_module.get_shape_suites()
    
    if not shape_suites:
        # Empty suite, use default
        result = eval_kernel_against_tilebench(
            reference_path, kernel_source, backend, kernel_name,
            config=None, warmup=warmup, repeat=repeat
        )
        results["default"] = result
        return results
    
    # Test on each configuration
    for i, config in enumerate(shape_suites):
        # Create readable config name
        if isinstance(config, dict):
            # Extract key parameters (exclude dtype, device)
            key_params = {k: v for k, v in config.items() if k not in ["dtype", "device"]}
            if key_params:
                config_name = "_".join(f"{k}{v}" for k, v in key_params.items())
            else:
                config_name = f"config_{i+1}"
        else:
            config_name = f"config_{i+1}"
        
        # Evaluate on this config
        result = eval_kernel_against_tilebench(
            reference_path,
            kernel_source,
            backend=backend,
            kernel_name=kernel_name,
            config=config,
            warmup=warmup,
            repeat=repeat,
        )
        
        results[config_name] = result
    
    return results


def get_aggregate_result(results: Dict[str, KernelEvalResult]) -> Dict[str, Any]:
    """
    Aggregate results from multiple configurations
    
    Args:
        results: Dict of config_name -> KernelEvalResult
        
    Returns:
        Dict with aggregate metrics
    """
    all_compiled = all(r.compiled for r in results.values())
    all_correct = all(r.correctness for r in results.values())
    
    # Speedup stats for correct results
    speedups = [r.speedup for r in results.values() if r.correctness and r.speedup > 0]
    
    return {
        "num_configs": len(results),
        "all_compiled": all_compiled,
        "all_correct": all_correct,
        "num_compiled": sum(1 for r in results.values() if r.compiled),
        "num_correct": sum(1 for r in results.values() if r.correctness),
        "speedup_mean": sum(speedups) / len(speedups) if speedups else 0.0,
        "speedup_min": min(speedups) if speedups else 0.0,
        "speedup_max": max(speedups) if speedups else 0.0,
        "configs": {
            name: {
                "compiled": r.compiled,
                "correct": r.correctness,
                "runtime_ms": r.runtime_ms,
                "speedup": r.speedup,
                "error": r.error_message,
            }
            for name, r in results.items()
        },
    }


if __name__ == "__main__":
    # Test with a simple Python implementation
    print("Eval CUDA against TileBench - Test")
    print("=" * 60)
    
    # Simple test kernel (PyTorch implementation)
    test_kernel = """
import torch

def run(A):
    '''RMSNorm reference implementation'''
    y = A / torch.sqrt(torch.mean(A**2, dim=-1, keepdim=True) + 1e-12)
    return y
"""
    
    import sys
    if len(sys.argv) > 1:
        ref_path = sys.argv[1]
    else:
        ref_path = "../../TileBench-Benchmark/basic/rmsnorm/reference.py"
    
    if os.path.exists(ref_path):
        print(f"\nTesting with: {ref_path}")
        
        # Test 1: Single default config
        print("\n--- Test 1: Default Config ---")
        result = eval_kernel_against_tilebench(
            reference_path=ref_path,
            kernel_source=test_kernel,
            backend="python",
        )
        
        print(f"Compiled: {result.compiled}")
        print(f"Correct: {result.correctness}")
        if result.runtime_ms > 0:
            print(f"Kernel time: {result.runtime_ms:.4f} ms")
            print(f"Reference time: {result.reference_runtime_ms:.4f} ms")
            print(f"Speedup: {result.speedup:.2f}x")
        
        if result.error_message:
            print(f"Error: {result.error_message}")
        
        # Test 2: All shape suites
        print("\n--- Test 2: All Shape Suites ---")
        all_results = eval_kernel_on_all_shapes(
            reference_path=ref_path,
            kernel_source=test_kernel,
            backend="python",
            warmup=2,
            repeat=5,
        )
        
        print(f"Tested on {len(all_results)} configurations:")
        for config_name, res in all_results.items():
            status = "✓" if res.correctness else "✗"
            speedup_str = f"{res.speedup:.2f}x" if res.speedup > 0 else "N/A"
            print(f"  {status} {config_name}: {speedup_str}")
        
        # Aggregate
        agg = get_aggregate_result(all_results)
        print(f"\nAggregate:")
        print(f"  All compiled: {agg['all_compiled']}")
        print(f"  All correct: {agg['all_correct']}")
        print(f"  Speedup mean: {agg['speedup_mean']:.2f}x")
        print(f"  Speedup range: [{agg['speedup_min']:.2f}x, {agg['speedup_max']:.2f}x]")
        
    else:
        print(f"Reference not found: {ref_path}")
        print("\nUsage:")
        print("  python -m src.eval_cuda_against_tilebench [reference_path]")

